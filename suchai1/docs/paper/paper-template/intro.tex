%!TEX root = adaptive_dict_kaf.tex
\section{Introduction}

Within kernel methods, kernel adaptive filters (KAFs) \cite{liu2010} are state-of-the-art nonlinear models for time series that build on the properties of reproducing kernel Hilbert spaces (RKHS) \cite{aronszajn1950theory}, in order to provide accurate predictions at a low computational cost. In the same way that support vectors play a fundamental role in support vector machines \cite{scholkopf01}, KAFs rely on a subset of observed input samples referred to as centres, where new inputs are compared to these centres through a kernel function to compute the prediction. This procedure involves a number of parameters: those of the kernel, those related to the selection of the set of centres (dictionary), and those controlling the trade-off between historical data and new observations. By adapting these model parameters, algorithms, such as kernel least mean square (KLMS) \cite{liu08,richard09} provide an efficient way to improve signal estimation over time as more data become available. Specifically, KLMS applies the least-mean-square rationale to the ``kernelised'' input (i.e., transformed by the kernel function), thus allowing for an efficient online implementation based on gradient steepest descent for updating the model parameters (i.e., the filter weights only).

The main drawback of KAFs is the lack of a principled approach to tune filter weights, kernel parameters and the dictionary. This is mostly due to the fact that the KAF approach is guaranteed to succeed, even when their parameters are not chosen carefully, owing to the universal approximation property of kernels \cite{steinwart01}. However, this may result on suboptimal implementations that, for instance, require large non-sparse dictionaries. Therefore, in our view, a theoretically-grounded parameter setting is required in order to achieve an efficient and accurate operation of KAFs. In \cite{tobar14mk} it was shown that different kernel widths can be used to predict wind-speed signals of different dynamic regimes, thus motivating the search for optimal parameters for the problem at hand; then, \cite{tobar_quat} provides a heuristic rule for parameter setting based on the histograms of the input samples. Another effort for parameter tuning within KAF has been achieved by relating KAFs to Gaussian processes (GP) \cite{rasmussen06}, where a GP-interpretation of the kernel recursive least squares (KRLS) tracker \cite{van2012kernel} allows for a probabilistic interpretation of KAF and therefore training. Additionally, online setting of dictionary and kernel hyperparameters have been studied by \cite{wang2015,chen2016kernel,fan2016kernel} using gradient-based optimisation. Alternatively, a variant of KAFs where parameter setting is straightforward due to preprocessing the observations can be found in \cite{dsp2017b}.

We propose a novel algorithm to completely train a kernel adaptive filter, that is, to find appropriate kernel parameters, weights and dictionary, using a probabilistic interpretation of KAFs. In addition to the probabilistic formulation that allows for training, our main contribution is the design of a sparsity-inducing prior distribution to determine an appropriate dictionary, which unlike standard KAFs is not restricted to be a subset of the observations. Our method can be implemented offline, where a subset of data is used to calculate initial conditions for the parameters of a standard KAF, or online, where a sliding window is used to perform recursive training and prediction.  We validate our approach using both illustrative examples with synthetic data and a real-world wind time series.

The paper is organised as follows: Section II gives a brief overview of KAFs and probabilistic inference; Section III presents the proposed methodology with an example using the Lorenz attractor time series; and Section IV shows the validation of the proposed method and compares it against standard KAFs. Finally, the conclusion and discussion of our findings are presented in Section V.